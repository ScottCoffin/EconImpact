---
title: "Economic Impacts Assessment"
author: "Scott Coffin"
date: "12/23/2020"
output:
  html_document:
    code_folding: hide
    theme: sandstone
    toc: yes
    toc_float: yes
---
<!-- # Setup -->
```{r setup, include=FALSE, echo = FALSE}
require("knitr")
knitr::opts_chunk$set(root.dir = '..')
knitr:: opts_chunk$set(message = FALSE)

#### Dependencies #####
library(tidyverse) #load dependencies
#library(BAMMtools) #jenks
#library(GmAMisc) #alt Jenks
library(cowplot) #plotting multiple graphs
library(rstatix) #pipe-friendly R fnx for stats
library(ggpubr) #easy plots
library(grid) #to make grobs
library(gridExtra) # to make multiple plots
library(calecopal) #colors
library(readxl) #to read in excel docs
library(survey) #survey analysis
#library(RCurl)
#library(MASS)
#library(glmnet)
library(mitools) #for mean imputation tools
library(mice)  # multiple imputation
library(foreign)
library(magrittr)
library(sampling) #to calculate probabilities 
library(DT) #datatable for nice printing
library(lattice)
library(PracTools) #for adjusting weights to non-response
library(RColorBrewer) #colors
library(pheatmap) #pretty heat maps
library(aod) #chi-square test
library(finalfit) #odds ratio plotting
library(NonProbEst) #nonprobability estimation: matchins
library(psych) #for making scatterplot matrices
library(car) #outliers
library(prettydoc)
library(BAMMtools) #jenki's breaks
options(scipen = 9999)
options(dplyr.width = Inf)

## resources
#https://bookdown.org/jespasareig/Book_How_to_weight_a_survey/nonresponse-weights.html
#https://rstudio-pubs-static.s3.amazonaws.com/278191_4285e2382468496e937d895251eafdf9.html #analyzing missind data in surveys
#https://github.com/amices/mice #mice package documentation
```

```{r data read, include=FALSE, echo = FALSE}
##### Read in Data #####
#excel doc contains individual spreadsheets for small/medium and large PWS'. Also one big spreadsheet for zip codes. Read in smalls/medium and large individually, but join both to zip
#just completed
smalls <- read_excel("Datasets/combined_results_w_pop.xlsx", sheet = "Small System Financial Qs") 
# #just completed
larges <- read_excel("Datasets/combined_results_w_pop.xlsx", sheet = "Large System Financial Qs") 
# #zip codes for joining
zip <- read_excel("Datasets/combined_results_w_pop.xlsx", sheet = "Combined Zip Table")
# #for comparing all data
# covid data for all systems
covid <- read_excel("Datasets/CWS_w_PWSID_COVID_UE_data.xlsx", sheet = "Sheet1") %>% 
  select(-c(Fee_Code, Service_Connections, Population, County, Regulating_Agency, Water_System_Name))
  
# AllPopConnections <- read_excel("Datasets/combined_results_w_pop.xlsx", sheet = "Population and Connections") 
# #ALL Community Water Systems
AllSystems <- read_excel("Datasets/CWS_fmt_PWSID.xlsx", na = "") 
#separate all larges for extrapolation
allLarges <- AllSystems %>% 
  filter(Service_Connections > 10000)
# ## Joined list of surveys requested, responded, voluntary with all response data generated from econ_impacts_plots_post_survey.R script
allSmalls <- read.csv("Datasets/AllSystems_Surveys_final.csv", stringsAsFactors = TRUE) %>%  filter(Service_Connections <10000) %>% 
  filter(Fee_Code !="WH") %>%  #filter out wholesalers 
  filter(Fee_Code !="N1") %>% #transient non-community water systems
  filter(Fee_Code !="N2") %>%  #transient non-community water systems (handwash exemption)
  filter(Fee_Code != "SP") #nonTransient-NonCommunity water system
#income data for all water systems and CES3.0 score
pop_weighted_characteristics <- read.csv("Datasets/pop_weighted_characteristics.csv", stringsAsFactors = TRUE)
#Risk scores calculated in python
risk_scores <- read_excel("Python/risk_scores.xlsx") %>% mutate_if(is.character, as.factor) %>% select(-c(Service_Connections, Population, Bin, months))

##### Cleanup and Join #####
# #responses for smalls
# smalls <- smalls %>% mutate_if(is.character, as.factor) 
# #responses for larges

#zip codes for all
zip <- zip %>% mutate_if(is.character, as.factor)
zip <- right_join(zip, pop_weighted_characteristics, by = "PWSID")
zip <- left_join(zip, risk_scores, by ="PWSID")
#join zip and income characteristics to smalls and larges
allSmalls <- left_join(allSmalls, zip) %>% distinct(PWSID, .keep_all = TRUE) %>%  droplevels()
# smalls <- left_join(smalls, zip, by = "PWSID") %>% distinct(PWSID, .keep_all = TRUE) #must clean dupes
allLarges <- left_join(allLarges, zip, by = "PWSID") %>% 
  distinct(PWSID, .keep_all = TRUE) %>% #must clean dupes
  droplevels() #unnecessary factors
allLarges %<>%  select(-c(cash:Risk_label)) #drop empty columns

##join all larges with surveyed larges
#cleanup duplicate columns in larges
larges %<>%  select(-c(sys_name, Population,`Service Connections`))
#join
allLarges %<>% left_join(larges, allLarges, by = "PWSID")

#recode non-respondents
allLarges$responded[is.na(allLarges$responded)] <- "n"
#provide tag for all
allLarges$tag[is.na(allLarges$tag)] <- "E"
#convert char to factors
allLarges %<>% mutate_if(is.character, as.factor)

## Join covid data to all
allSmalls <- left_join(allSmalls, covid, by = "PWSID") %>% 
  distinct(PWSID, .keep_all = TRUE) %>% 
  droplevels()

allLarges <- left_join(allLarges, covid, by = "PWSID") %>% 
  distinct(PWSID, .keep_all = TRUE) %>% 
  droplevels()


# #keep it clean
rm(zip)
rm(pop_weighted_characteristics)
rm(risk_scores)
```
<!-- ## Recode Variables -->
<!-- Now that we've read in the data, let's recode some factor variables as either dichotomous or continuous index variables. -->
```{r recode variables, include=FALSE, echo = FALSE}
#turn months before assistance from categorical to continuous variable
allSmalls %<>%  mutate(months_before_assist_num = case_when(months_before_assist == "A" ~ 1,
                                                           months_before_assist == "B" ~ 2,
                                                           months_before_assist == "C" ~ 3,
                                                           months_before_assist == "D" ~ 4,
                                                           months_before_assist == "E" ~ 5,
                                                           months_before_assist == "F" ~ 6)) %>% 
  mutate(fee_code_char = as.character(Fee_Code)) #recode for non-factor processes
#provide integer tag for sampled
allSmalls %<>% 
  mutate(response = case_when(responded == "y" ~ 1,responded == "n" ~ 0))
#normalize number of delinquent accounts to number of service connections
allSmalls %<>% 
  mutate(delinquent_num_acc_normalized = delinquent_num_acc/Service_Connections)
#provide integer tag for voluntary status  
allSmalls %<>% 
  mutate(volunteered = case_when(voluntary == "y" ~ 1,voluntary == "n" ~ 0))
#provide integer tag for loan status
allSmalls %<>% 
  mutate(loans_1.0 = case_when(loans_YN == "Y" ~ 1,loans_YN == "N" ~ 0)) 
#provide integer tag for submetered status
allSmalls %<>% 
  mutate(submetered_1.0 = case_when(submetered_YN == "Y" ~ 1,submetered_YN == "N" ~ 0)) 
#useful for comparisons for voluntary data
allSmalls.requested.voluntary <- allSmalls %>% filter(voluntary == "n" |voluntary == "y" | voluntary == "no.response")
# Create dataset of survey list with both respondents and non-respondents, but NOT volunteers (not request but responded)
allSmalls.requested.responded <- allSmalls %>%  filter((voluntary == "n"))
#Separate volunteer samples for comparison
allSmalls.volunteers <- allSmalls %>%  filter((voluntary == "y"))
#Dataset for all responses (volunteers and requested)
allSmalls.responded <- allSmalls %>% 
  filter(responded == "y")
#just requested
allSmalls.requested <- allSmalls %>% 
  filter(requested == "y")
```

# Blending Probability and Non-Probability Samples

Note that some systems (n = 39 out of 410) that were not requested to participate in the survey did. This is due to some district staff calling systems that were not included in their lists. NOTE: these systems were not 'voluntary' *per se*, but were rather *non-probabilistic*, however in this document, they are referred to as *voluntary* for short-hand.

A table displaying the number and proportions of non-probabilistic systems that were included in the survey is below.
```{r}
#wide format for table viewing
allSmalls %>% group_by(tag) %>% filter(voluntary == "n" | voluntary == "y") %>%  summarize(percent.voluntary = sum(volunteered)/n()*100, volunteered.sum = sum(volunteered))
```

These data can also be visualized in a plot. Further annotated with the non-probabilistic responses received.
```{r}
#plot
allSmalls %>% filter(voluntary == "n" | voluntary == "y") %>% 
  ggplot(aes(x = tag, fill = voluntary)) +
  geom_bar(position = position_stack(reverse = TRUE))+
  scale_fill_manual(values = cal_palette("superbloom2"), name = "Sample Type", labels = c("Probabilistic", "Non-probabilistic")) +
  scale_x_discrete(name = "Sampling Bin") +
  scale_y_continuous(name = "Responded") +
  labs(title = "Proportion of Probabilistic and Non-Probabilistic Samples by Sampling Bin")+
  theme_minimal()
```

A clear trend of more non-probabilistic responses (referred to as *voluntary*) from the smaller systems (Bins A and B) is visualized in the above figure. To determine if these samples are significantly different from the *probabilistic* samples collected by District staff, a number of tests will be run below. The first of which is a general linear model with a binomial distribution to determine if there are significant relationships between their probabilistic/non-probabilistic status and some auxiliary and response variables, including cash reserves, median 12-month income, number of service connections, and months before assistance.

```{r}
# build model
volunteerGLM <- glm(volunteered ~ cash_days + cash + Median_12month_HH_income + Service_Connections + months_before_assist, na.action = na.omit,
            data = allSmalls.responded, family = "binomial")
#print 
summary(volunteerGLM)
```

There is a nearly significant (*p* = 0.0501) correlation between median 12 month household income and probability to volunteer for the survey, and extremely significant correlation between number of service connections  (*p* = 0.0003) as well as months before assist groups E and F (greater than 12 months and no financial response anticipated, respectively). These trends are likely due to the disproportionate abundance of small systems for which there are *voluntary* samples.

```{r}
p1 <- allSmalls.responded %>% droplevels()

cdplot(voluntary ~ Service_Connections, data =p1,
       main = "Conditional Density Plot of Volunteer Status for Survey", xlab = "Number of Service Connections", ylab = "Non-probabilistic?")
```

In the figure above, it is easy to visualize the extremely significant inverse trend between number of service connections and non-probabilistic status in the survey dataset. These proportions may be further visualized by fee code, below.

```{r}
#plot
allSmalls %>% filter(voluntary == "n" | voluntary == "y") %>% 
  ggplot(aes(x = Fee_Code, fill = voluntary)) +
  geom_bar(position = "fill")+
  scale_fill_manual(values = cal_palette("superbloom1"), name = "Sample Type", labels = c("Probabilistic", "Non-probabilistic")) +
  scale_x_discrete(name = "Fee Code") +
  scale_y_continuous(name = "Non-probabilistic Proportion", labels = scales::percent) +
  labs(title = "Proportion of Probabilistic and Non-Probabilistic Samples by Fee Code")+
  theme_minimal()
```

The figure above shows that the majority of non-probabilistic samples are in the "smalls" fee code categories (DAVCS and CS).

Since the non-probabilistic samples are not equally distributed amongst classes, corrections are necessary to blend them with the probabilistic samples. Since census information is available for the population (i.e. service connections, fee code, districts, etc.), these samples may still be used for further analysis by sub-sampling a representative sample from this list using *sample matching* methods. The theory for matching nonprobability volunteer samples with probability samples is described by [Rubin (1979)](https://www.jstor.org/stable/2286330).




# Survey Completeness

Since data are missing for some categories within responses, we have three choices:
1) listwise-deletion: remove rows that contain missing data. This will, of course, reduce the strength of the dataset.
2) mean/median substitution: another quick fix that takes the mean/median of the existing data points and substitutes the missing data points. This would obviously bias the analysis since it decreases variance. 
3) Multiple imputation: With this approach, rather than replacing missing values with a single value, we use the distribution of the observed data/variables to estimate multiple possible values for the data points. This allows us to account for the uncertainty around the true value, and obtain approximately unbiased estimates (under certain conditions). Moreover, accounting for uncertainty allows us to calculate standard errors around estimations, which in turn leads to a better sense of uncertainty for the analysis.

To decide which option to use, let's first examine how much data is missing from our survey.
```{r}
#make subset of data showing complete cases (rows that have full coverage of variables) for the systems that COMPLETED the survey and were asked to participate (i.e. no volunteers).
sub <- allSmalls.requested.responded %>% select(cash_reserve_unrestricted, cash_reserve_restricted, cash_reserve_total, months_before_assist, delinquent_num_acc, delinquent_amount_dollars)
#how many NA's are present total
#sum(is.na(sub))
#how many complete cases total
#sum(complete.cases(sub))
sum(complete.cases(sub)) / nrow(sub)
```
We can see that a rather large proportion (41%!) of survey data is missing just for the selected few variables, which are considered to be the primary responses of interest in this survey. If we were to simply exclude incomplete cases, we would trim a substantive poriton of the dataset, potentially reducing the power of the study to unnacceptable levels. The table below takes a deeper look at the number of missing values in each column for the probabilistic and non-probabilistic samples. 

```{r}
rm(sub) #keep it clean

#create dataframe with completeness
completenessSummary <- allSmalls %>%
  filter(voluntary == "y" |voluntary == "n") %>%
  select(voluntary, cash_reserve_unrestricted, cash_reserve_restricted, cash_reserve_total, months_before_assist, delinquent_num_acc, delinquent_amount_dollars) %>%
  group_by(voluntary) %>%
   summarise_all((name = ~sum(is.na(.))/length(.))) 
#convert to matrix and transpose
transposed <- as.data.frame(t(as.matrix(completenessSummary[2:7])))
#reassign column names
colnames(transposed) <- c("Probabilistic", "Non-Probabilistic")
transposed
```
There are no obvious differences in response completeness between the probabilistic and non-probabilistic samples, as demonstrated in the above table. This provides reassurance that blending these samples is viable. Further, District Staff report that small systems often did not report whether cash was restricted or unrestricted due to the lack of such designations or restrictions in their accounting systems. Therefore, it is a reasonable assumption that missing values for restricted cash reserve may be recorded as zero. This correction takes place in the below code chunk.

```{r}
#recode restricted cash reserves to 0
allSmalls$cash_reserve_restricted[is.na(allSmalls$cash_reserve_restricted)] <- 0

#create dataframe with completeness with restricted cash reserve question as 0
completenessSummary <- allSmalls %>%
  filter(voluntary == "y" |voluntary == "n") %>%
  select(voluntary, cash_reserve_unrestricted, cash_reserve_restricted, cash_reserve_total, months_before_assist, delinquent_num_acc, delinquent_amount_dollars) %>%
  group_by(voluntary) %>%
   summarise_all((name = ~sum(is.na(.))/length(.))) 
#convert to matrix and transpose
transposed <- as.data.frame(t(as.matrix(completenessSummary[2:7])))
#reassign column names
colnames(transposed) <- c("Probabilistic", "Non-Probabilistic")
transposed

#format as matrix
MissingMatrix <- data.matrix(transposed)
#build heatmap
pheatmap(MissingMatrix,
                           main = "Missing Responses for Surveyed Systems", #title
                           fontsize = 12,
                           cluster_rows = FALSE, cluster_cols = FALSE,#disable dendrograms
                           display_numbers = TRUE,
                           treeheight_row = 0, treeheight_col = 0, #keeps clustering after dropping dendrograms
                           col = brewer.pal(n = 9, name = "PuBu")) #blue color scheme with 9 colors)
```
It's clear that both the probabilistic and non-probabilistic surveyed water systems provided similiar levels of information in this survey. Let's see if there were differences in response completeness by sampling bin. 

```{r}
tagCompletenessSummary <- allSmalls %>%
  filter(voluntary == "y" |voluntary == "n") %>%
  select(tag, cash_reserve_unrestricted, cash_reserve_restricted, cash_reserve_restricted, cash_reserve_total, months_before_assist, delinquent_num_acc, delinquent_amount_dollars) %>%
  group_by(tag) %>%
   summarise_all((name = ~sum(is.na(.))/length(.))) %>% 
  mutate(across(is.numeric,~round(., 2))) 
tagCompletenessSummary
```
Again, these data may be easier to view as a heatmap.
```{r}
#convert to matrix and transpose
transposedTag <- as.data.frame(t(as.matrix(tagCompletenessSummary[2:7]))) #1 is category, 2-6 are variables
#reassign column names
colnames(transposedTag) <- c("Bin A", "Bin B", "Bin C", "Bin D")
transposedTag
#format as matrix
MissingMatrixTag <- data.matrix(transposedTag)
#build heatmap
pheatmap(MissingMatrixTag,
                           main = "Data Missing by Bin", #title
                           fontsize = 12,
                           cluster_rows = FALSE, cluster_cols = FALSE,#disable dendrograms
                           display_numbers = TRUE,
                           treeheight_row = 0, treeheight_col = 0, #keeps clustering after dropping dendrograms
                           col = brewer.pal(n = 9, name = "PuBu")) #blue color scheme with 9 colors)
```
There seems to be a clear step-wise trend for the willingness to reportunresicted cash reserve by size (Bin A are smallest, Bin D are largest). This tracks with the above statement regarding District Staff reponses from surveys. Furthermore, unrestricted cash reserves are not applicable to small systems that do not have restrictions, so these missing values are not an issue. Luckily, no other obvious trends for missing response values appear to be present. Let's see if there's a relationship between reporting of this value and service connections.

```{r}
rm(MissingMatrixTag, tagCompletenessSummary, completenessSummary, transposed, transposedTag)
#convert numeric to 1
allSmalls.responded <- allSmalls.responded %>% 
  mutate(cash_reserve_restricted_NA = case_when(cash_reserve_restricted >= 0 ~ 1))

#Convert NA to 0
allSmalls.responded$cash_reserve_restricted_NA <- allSmalls.responded$cash_reserve_restricted_NA %>% replace_na(0) %>% as.factor()
#convert to factor
allSmalls.responded <- allSmalls.responded %>% 
  mutate(cash_reserve_restricted_NA_yn = case_when(cash_reserve_restricted_NA == "0" ~ "no",
                                                   cash_reserve_restricted_NA == "1" ~ "yes")) 
allSmalls.responded$cash_reserve_restricted_NA_yn <- as.factor(allSmalls.responded$cash_reserve_restricted_NA_yn)

#examine relationship of response to restricted reseved cash question with number of  service connections

lgit<- glm(cash_reserve_restricted_NA ~ Service_Connections + Fee_Code, 
            data = allSmalls.responded, family = "binomial")
summary(lgit)
```
Here we can see that a significant relationship exists between number of service connections and the response level for the question regarding restricted cash reserves. There also seems to be a significant relationship between the disadvantaged community larges, which is related to service connections.

This may also be visualized in a conditional density plot.
```{r}
cdplot(cash_reserve_restricted_NA_yn ~ Service_Connections, data = allSmalls.responded,
       main = "Conditional Density Plot of Response for Restricted Cash Reserves", xlab = "Number of Service Connections", ylab = "Reponse Given for Restricted Cash Reserves?")
```

Let's further test these grouping differences in response with a chi-squared test.  
```{r}
wald.test(b = coef(lgit), #coefficients from logit model 
          Sigma = vcov(lgit), #variance covariance matrix of the error terms
          Terms = 3:5) #categorical terms (fee codes)
```

The p-value of 0.0035 again suggests that fee code is a significant predictor for willingness to respond to this question. An easier way to conceptualize this may be through odds-ratios.

```{r,  }
#exponentiate the coefficients (i.e. create odds-ratios)
round(exp(cbind(OddsRatio = coef(lgit), confint(lgit))),2)
```

Now we can clearly see that disadvantaged community Larges are 3.47 times more likely to report their unrestricted cash reserves (97.5% CI: 1.83 - 6.84). Odds ratios are plotted below:

```{r}
#remove model 
rm(lgit)
#define explanatory variables
explanatory = c("Service_Connections", "Fee_Code")
#plot ORs
allSmalls.responded %>% 
  or_plot("cash_reserve_restricted_NA",
          explanatory, table_text_size=4, title_text_size=14, 
    plot_opts=list(xlab("OR, 95% CI"), theme(axis.title = element_text(size=12))))
```

Since we've determined (both statistically, and qualitatively from District Staff responses) that the reporting of restricted/unrestricted cash reserves is dependent on accounting systems that are reflective of the system's relative size, a reliable assumption may be made that total cash reserves can be expressed as unrestricted cash reserves for systems that only report totals and no restriction status. This simple conversion will allow for meaningful comparisons and reduce the amount of missing data while retaining high confidence by avoiding the need for sophisticated approaches (e.g. imputation) that would introduce additional uncertainty.

A simple logical test is used to create a new variable called "cash_reserve_aggregate", which is equivalent to total cash reserves if no information is provided for restricted cash reserves, or unrestricted cash reserves if information is provided for restricted cash reserves.

```{r recode restricted cash reserves}
#recode with new variable
allSmalls.responded %<>% 
  mutate(cash_reserve_aggregate = ifelse(cash_reserve_restricted_NA_yn == "no", cash_reserve_total, cash_reserve_unrestricted))
```

The filling in of missing data from the above conversion may be visualized in the heatmap below. 

```{r}
#create table
tagCompletenessSummary <- allSmalls.responded %>% 
  select(tag, cash_reserve_aggregate, cash_reserve_unrestricted, cash_reserve_restricted, cash_reserve_restricted, cash_reserve_total, months_before_assist, delinquent_num_acc, delinquent_amount_dollars) %>%
  group_by(tag) %>%
   summarise_all((name = ~sum(is.na(.))/length(.))) %>% 
  mutate(across(is.numeric,~round(., 2))) 

#convert to matrix and transpose
transposedTag <- as.data.frame(t(as.matrix(tagCompletenessSummary[2:8]))) #1 is category, 2-6 are variables
#reassign column names
colnames(transposedTag) <- c("Bin A", "Bin B", "Bin C", "Bin D")
transposedTag
#format as matrix
MissingMatrixTag <- data.matrix(transposedTag)
#build heatmap
pheatmap(MissingMatrixTag,
                           main = "Data Missing by Bin", #title
                           fontsize = 12,
                           cluster_rows = FALSE, cluster_cols = FALSE,#disable dendrograms
                           display_numbers = TRUE,
                           treeheight_row = 0, treeheight_col = 0, #keeps clustering after dropping dendrograms
                           col = brewer.pal(n = 9, name = "PuBu")) #blue color scheme with 9 colors)
```

In the above heatmap, it can be seen that the the newly created variable "cash_reserve_aggregate" has virtually the same amount of missing data as the total cash reserve variable, thus confirming that the conversion process was successful.

# Survey Weighting
Survey weights are a key component to producing population estimates. As described in Valliant et al. 2018, an estimated total has the form $t = \sum_{s} w_i y_i$ where $y_i$ is a response provided by the *i*th sample member and $w_i$ is the corresponding analysis weight. Without the use of weights, estimates from survey data may simply reflecy nuances of a particular sample, containing significant levels of bias. The series of weighting that will be applied to this survey data include computation of base weights, nonresponse adjustments, and use of auxiliary data to reduce variances (i.e. calibration).

## Base Weights
The first basic step to make reliable extrapolations to the population is to generate base weights. Note that this is one of three weight adjustments that will be applied,including non-response and calibration adjustments to weights.
The first step in weighing is taking into account the different probabilities of being sampled that respondents may have simply based on proportions in the population. This is also known as generating *base weights*. Note that this survey is a random stratified sampling design without replacement. We will calculate inclusion probabilities for each strata for the small systems (tags A, B, C, D). Large systems (>10,000 service connections) will be treated as a single strata (E) for the calculation of base weights.  

```{r calculate probabilities and base weights for small systems}
probSumm <- allSmalls %>% 
  filter(voluntary != "y") %>% #remove volunteers
  mutate(sample = case_when(requested == "y" ~ 1,requested == "n" ~ 0)) %>% #provide integer tag for sampled
  group_by(tag) %>% 
  summarize(N = n(),samples = sum(sample), respondedTotal = sum(response, na.rm = TRUE), prob = samples/N, base.weight = 1/prob, 
            sum.base.weight = base.weight * samples, fpc = samples/N) %>% 
  drop_na
#join the probability to each sample
allSmalls <- right_join(allSmalls, probSumm, by = "tag")
allSmalls.requested.responded <- right_join(allSmalls.requested.responded, probSumm, by = "tag")
#print
probSumm
```

Note in the code above that *finite population correction* (fpc) is defined as the value of the sampling rate, *n/N* - not *1 - n/N* - which is the textbook definition of the *fpc*. This is an idiosyncracy of the *survey* package in R and is consistent with other programming languages such as Stata or SAS. 

We can see here that the probability of being chosen within each strata is slightly different. A basic but important test that should be performed after computing the probabilities is making sure that all probabilities are between 0 and 1.

We will also calculate the scaled base weights, which should add up to the total number of respondents.

```{r, results= 'hide'}
scaledSumm <- allSmalls.requested.responded %>% 
  group_by(tag) %>% 
  summarize(n = n(),  scaled.base.weight = base.weight/N*(samples), sum.scaled.base.weight
            = sum(scaled.base.weight))

allSmalls %>% 
  mutate(scaled.base.weight = 1)

allSmalls.requested.responded %>% 
  mutate(scaled.base.weight = 1)
#print
head(scaledSumm)
```

Here we can see that the scaled base weights are nominal (i.e. 1), and all add up to the respondent sample size for each bin. Later we will calcualted non-response weights and multiply them by the scaled base weights.
  
```{r}
rm(probSumm, scaledSumm)
probabilities <- allSmalls %>%
  group_by(tag) %>% 
  summarise(min.probability = min(prob, na.rm = T),
            mean.probability = mean(prob, na.rm = T),
            max.probability = max(prob, na.rm = T)) %>%
  as.vector()
print(probabilities)
```
```{r}
#check to ensure probabiliities are realistic
#if(probabilities$min.probability < 0){stop("Minimum probability of being sampled is smaller than 0. Review sampling probabilities before computing base weights.")}else if(probabilities$max.probability > 1){stop("Maximum probability of being sampled is larger than 1. Review sampling probabilities before computing base weights.")}
rm(probabilities)
```

There may be underlying factors that make some types of water systems more or less likely to be sampled. For instance, let's look at the mean probabilities by fee code.

```{r}
allSmalls %>%
  filter(!is.na(prob)) %>%
  group_by(tag, Fee_Code) %>%
  summarise(n = n(),
    mean.prob.percentage = mean(prob, na.rm = T)*100) %>%
      arrange(desc(mean.prob.percentage)) %>% 
  head()
```
Since some fee codes roughly fit within sampling bins (strata), it makes sense that we see discrete groupings. Other factors such as demographics may have other probabilities of being sampled. Let's see what the difference in probabilities are for months before assistance, which has six discrete levels (A-F).
```{r}
allSmalls %>%
  group_by(months_before_assist, tag) %>%
  filter(!is.na(months_before_assist)) %>% 
  summarise(n = n(),mean.prob.percentage = mean(prob, na.rm = T)*100) %>%
      arrange(desc(mean.prob.percentage)) %>% 
  head()
```
We can see there are minor, albeit seemingly significant differences in probabilities for sampling by months before assist. 

To correct for these differential probabilities, we must design weights (sometimes called base weights) so that our sample does not over- or under-represent relevant groups. The design weights are equal to the inverse of the probability of inclusion to the sample. Therefore, the design weight (d0) of a respondent (i) will be equal to: d0[i] = 1/pi[i] where pi[1] is the probability of that unit being included in the sampling.

A simple interpretation of design weights is ‘the number of units in our population that each unit in our sample represents’. There is a simple but important test that we should perform after computing design weights. **The sum of all design weights should be equal to the total number of units in our population.** 

Now to ensure design weights sum up to the entire population from which each population (bin) was drawn, we repeat the code from above for all smalls. 
```{r}
allSmalls %>% 
  mutate(sample = case_when(requested == "y" ~ 1,requested == "n" ~ 0)) %>% #provide integer tag for sampled
  group_by(tag) %>% 
  summarize(n = n(),samples = sum(sample), prob = samples/n, base.weight = 1/prob, 
            sum.base.weight = base.weight* samples) %>% 
  drop_na
```


Below these steps are briefly repeated for large systems (greater than 10,000 service connections).
```{r calculate probabilities and base weights for large systems}
probSumm <- allLarges %>% 
  mutate(sample = case_when(responded == "y" ~ 1,responded == "n" ~ 0))  %>% #provide integer tag for sampled
  group_by(tag) %>% 
  summarize(N = n(),samples = sum(sample), prob = samples/N, base.weight = 1/prob,
            sum.base.weight = base.weight * samples, fpc = samples/N)
#join the probability to each sample
allLarges <- right_join(allLarges, probSumm, by = "tag")
#print
probSumm
```


## Response Propensity Adjustment
Now that we have designed our basic weights, we now need to account for differences in the propensity to respond. It's possible that certain profiles (e.g. lower income communities) had different propensities to respond than another profile (e.g. higher income communities). We could then imagine that the characteristics of both profiles were associated with reponse variables that we collected in this survey (e.g. water systems with lower income having more delinquent accounts than higher income systems). As we then would have a larger proportion of higher income/fewer delinquent accounts in our sample, our analyses would be biased.

First let's see the response rate for each of the bins.
```{r}
allSmalls %>% group_by(tag) %>% filter(requested == "y") %>% 
  summarize(completeness = sum(response)/n() * 100 , completed = sum(response), total = n())
```


Computing the probability of replying to this survey is challenging because we can not directly observe the probability of replying to the survey, therefore we need to estimate it. This may be done using information which we know for both respondent and non-respondent units. The most reliable (albeit most complicated) of calculating non-response probabilities is through predictive modelling.

[Valliant et al (2013)](https://link.springer.com/book/10.1007%2F978-1-4614-6449-5) recommend estimating the response propensities and then grouping them in classes. The grouping step should avoid extreme weights. One way of estimating the response propensities is using logistic regression. This logistic regression should be unweighted. We will use a general linear model function to predict non-responses with hypothesized response variables for which we have data, including service connections, population, fee code, and regulating agency.
```{r modeling response propensity}
#trim data to those in sample list with auxiliary data
allSmalls.requested.response.model <- allSmalls %>% 
  filter(requested == "y") %>% 
  select(response, Service_Connections, Population, CES_3.0_Score, Median_12month_HH_income,
         Median_rent_pct_income) %>% 
  drop_na()

#prepare formula
formula.resp <- as.formula("response ~ Service_Connections + Population + CES_3.0_Score + Median_12month_HH_income + Median_rent_pct_income")
options(na.action = 'na.pass')

#format as matrix
x.matrix <- model.matrix(formula.resp , data = allSmalls.requested.response.model)[, -1]

#fit binomial model
log.reg.m <- glm(formula.resp, data = allSmalls.requested.response.model,family = "binomial")

coef.response.log <- coef(log.reg.m)
predicted.log <- log.reg.m$fitted.values
non.responsepredicted.log <- predicted.log

predicted.log %>% head()
```
The above output shows the first six estimated response propensities of our dataset. Since we don't have pardata for all samples, we are now computing our predictor estimates from a subset of sampled units.

Let's define our survey structure in the *survey* package. 
```{r survey initiate, results='hide'}
##### Specify sampling design ####
#here we will use our imputed survey dataset
#(stratified)
srv.dsgn <- svydesign(data = imputedSmalls, 
                        weights = ~base.weight, #if weighted. Don't forget tilde ~
                        fpc = ~fpc, #population size for each stratum
                        strata = ~tag, #bins
                        id = ~1) #specifies independent sampling (without replacement)
#specify replicate survey design
rclus1 <- as.svrepdesign(srv.dsgn)
#print summary of design
summary(srv.dsgn)
```

The package *PracTools* has tools to determine propensity classes. We will use service connections, population, fee code, and regulating agency as predictor variables.

First let's start by testing which variables are predictive of response. 
```{r}
#redefine all smalls
allSmalls.requested <- allSmalls %>% filter(requested == "y")
#fit
fitResponse <- lm(response ~ Service_Connections + Population + Median_12month_HH_income + Median_rent_pct_income + CES_3.0_Score + Fee_Code, na.action = na.omit, data = allSmalls.requested)
summary(fitResponse)
```
Here we can see that the number of service connections and population are good predictors for response propensity, and possibly fee code (DAVCL). This is convenient, because we know these values for all systems. We will now define classes using this parameters using a logistic regression. 
```{r}
#be sure to use the dataset with non-response data
out <- pclass(formula = response ~ Service_Connections + Population + Fee_Code,
              data = allSmalls.requested,
              type = "unwtd", #already have base weights
              link = "logit",
              numcl = 4) #classes to create
table(out$p.class, useNA = "always") # ensures no unit has a missing class value
```
Further examine propensities.
```{r}
summary(out$propensities)
```
These propensity classes can be viewed in boxplot form.
```{r}
boxplot(out$propensities ~ out$p.class)
```
We can see discrete propensity classes with rather tight spread for the four uppermost categories. We see a string of outliers in the lowest class. We have several options to adjust our weights for non-response. These include multiplying the input weights by the inverse of cell response propensities. Let's further examine these options.

```{r}
cbind(
  "mean" = by(data = out$propensities, INDICES = out$p.class, FUN = mean),
  "median" = by(data = out$propensities, INDICES = out$p.class, FUN = median),
  "weighted" = by(data = data.frame(preds = out$propensities, wt = allSmalls.requested[,"base.weight"]), out$p.class, function(x) {weighted.mean(x$preds, x$wt)}))
```
We can see that these are all quite similiar. So weighting by the base weights may not be necessary or distinct. Just to ensure, let's perform a check on covariate balance by fitting an ANOVA model to service connections, which is continuous. We do not use the survey weights below since the interest is in whether balance has been achieved in the sample that was selected. Checks could be made using the weights, in which case the check would be on whether the census-fit model shows evidence of balance.
```{r}
#extract classes
p.class <- out$p.class
#build glm
chk1 <- glm(Service_Connections ~ p.class + response + p.class*response,
data = allSmalls.requested)
#print
summary(chk1)
```
In this case, the *p.class* factors all have coefficients that are significant while the p.class*resp interactions are not—the desired outcomes if mean service connections differs between classes but is the same for respondents and nonrespondents within a class. Another check is to fit a second model that includes only
*p.class* and to test whether the models are equivalent:
```{r}
chk2 <- glm(Service_Connections ~ p.class, data = allSmalls.requested)
anova(chk2, chk1, test="F")
```
The F-statistic is 0.647 with 506 and 502 degrees of freedom and has a p-value of 0.63. Thus, the model without a factor for responding is judged to be adequate.

### Classification Algorithms = CART
In the nonrespondent application, the decision tree will classify cases using available covariates into classes that are related to their likelihood of being respondents. Advantages of CART compared to propensity modeling are that: 
1. Interactions of covariates are handled automatically.
2. The way in which covariates enter the model does not have to be made
explicit.
3. Selection of which covariates and associated interactions should be included
is done automatically.
4. Variable values, whether categorical or continuous, are combined (grouped)
automatically.
```{r}
require(rpart)
set.seed(15097)
t1 <- rpart(response ~ Service_Connections + Population + Fee_Code +  Median_rent_pct_income +  Median_12month_HH_income,
            method = "class",
            control = rpart.control(minbucket = 20, cp=0), #requires that there be at least 19 cases (responded + nonrespondents) in the final grouping of variable values of the terminal node of the tre..
            data = imputedSmalls.requested.voluntary)
print(t1, digits=4)
```
Plot an interpretable tree.
```{r}
require(rpart.plot)
cols <- ifelse(t1$frame$yval == 1, "gray50", "black")
prp(t1, main="Tree for NR adjustment classes (includes imputed values)",
    extra=106, # display prob of survival and percent of obs
    nn=TRUE, # display node numbers
    fallen.leaves=TRUE, # put leaves on the bottom of page
    branch=.5, # change angle of branch lines
    faclen=0, # do not abbreviate factor levels
    trace=1, # print automatically calculated cex
    shadow.col="gray", # shadows under the leaves
    branch.lty=1, # draw branches using solid lines
    branch.type=5, # branch lines width = weight(frame$wt), no. of cases here
    split.cex=1.2, # make split text larger than node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    col=cols, border.col=cols, # cols[2] if survived
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=0.5) # round the split box corners a tad
```
The tree can now be clearly visualized. This exercise will be repeated for the *non-imputed* samples without volunteers.
```{r}
require(rpart)
set.seed(15097)
t2 <- rpart(response ~ Service_Connections + Population + Fee_Code +  Median_rent_pct_income +  Median_12month_HH_income,
            method = "class",
            control = rpart.control(minbucket = 20, cp=0), #requires that there be at least 19 cases (responded + nonrespondents) in the final grouping of variable values of the terminal node of the tre..
            data = allSmalls.requested)
print(t2, digits=4)
```
Plot an interpretable tree.
```{r}
require(rpart.plot)
cols <- ifelse(t2$frame$yval == 1, "gray50", "black")
prp(t2, main="Tree for NR adjustment classes (no imputed values)",
    extra=106, # display prob of survival and percent of obs
    nn=TRUE, # display node numbers
    fallen.leaves=TRUE, # put leaves on the bottom of page
    branch=.5, # change angle of branch lines
    faclen=0, # do not abbreviate factor levels
    trace=1, # print automatically calculated cex
    shadow.col="gray", # shadows under the leaves
    branch.lty=1, # draw branches using solid lines
    branch.type=5, # branch lines width = weight(frame$wt), no. of cases here
    split.cex=1.2, # make split text larger than node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    col=cols, border.col=cols, # cols[2] if survived
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=0.5) # round the split box corners a tad
```


### Classification 
A single regression tree does tend to overfit the data in the sense of creating a model that may not be accurate for a new dataset (like the units that were not sampled or another sample selected using the same methods that is also subject to nonresponse). For a nonresponse adjustment, the fitted model from a single tree may not be the best representation of the underlying response mechanism. Breiman (2001) formulated random forests as a way of creating predictions that suffer less from this “shrinkage” problem. Random forests
fit many regression trees and average the results with the goal of producing more robust, lower variance predictions. Random forests can incorporate a large number of weighting variables and can find complicated relationships between adjustment variables that a researcher may not be aware of in advance( [Zhao et al. 2016](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4818178/)).

```{r, results='hide'}
require(party)
crf.srvy <- cforest(as.factor(response) ~ Service_Connections + Fee_Code + Population + Median_rent_pct_income +  Median_12month_HH_income, 
                    controls = cforest_control(ntree = 500, 
                                               mincriterion = qnorm(0.8), 
                                               trace = TRUE), # adds project bar because it's very slow
                    data=imputedSmalls.requested.voluntary)

crfsrvy.prob <- predict(crf.srvy,newdata=imputedSmalls.requested.voluntary,type="prob")
rpart.prob <- predict(t1, newdata=imputedSmalls.requested.voluntary,type="prob")
crf.prob <- matrix(unlist(crfsrvy.prob), ncol=2, byrow=TRUE)
apply(crf.prob,2,mean)
#[1] 0.2524514 0.7475486
tab <- round(cbind(by(rpart.prob[,2], INDICES=t1$where, mean), by(crf.prob[,2], INDICES=t1$where, mean)), 4)
colnames(tab) <- c("rpart", "cforest")
tab
```
The estimated overall response rate is 0.748. This is very close to the actual overall response rate of 0.7468. Above we can see the different propensity classes predicted by the rforest and the cforest.
```{r, results= 'hide'}
require(party)
crf.srvy.2 <- cforest(as.factor(response) ~ Service_Connections + Fee_Code + Population + Median_rent_pct_income +  Median_12month_HH_income, 
                    controls = cforest_control(ntree = 500, 
                                               mincriterion = qnorm(0.8), 
                                               trace = TRUE), # adds project bar because it's very slow
                    data=allSmalls.requested)

crfsrvy.prob2 <- predict(crf.srvy.2,newdata = allSmalls.requested,type="prob")
rpart.prob2 <- predict(t2, newdata = allSmalls.requested,type="prob")
crf.prob2 <- matrix(unlist(crfsrvy.prob2), ncol=2, byrow=TRUE)
apply(crf.prob2,2,mean)
#[1] 0.2711782 0.7288218
tab <- round(cbind(by(rpart.prob2[,2], INDICES=t2$where, mean), by(crf.prob2[,2], INDICES=t2$where, mean)), 4)
colnames(tab) <- c("rpart (no imputation)", "cforest (no imputation)")
tab
```

Adjusted weights can now be computed and merged into the data file. 
```{r}
##### join to non-imputed data ####
# compute NR adjustments based on classes formed by tree
# Unweighted response rate
unwt.rr <- by(as.numeric(allSmalls.requested[, "response"]), t2$where, mean)
# Weighted response rate
wt.rr <- by(data = data.frame(resp = as.numeric(allSmalls.requested[, "response"]), wt =
                                allSmalls.requested[,"base.weight"]),
            t2$where, function(x) {weighted.mean(x$resp, x$wt)} )
# merge NR class and response rates onto  file
allSmalls.requested.NR <- cbind(allSmalls.requested, NR.class=t2$where)
tmp2 <- cbind(NR.class=as.numeric(names(wt.rr)), unwt.rr, wt.rr)
allSmalls.requested.NR <- merge(allSmalls.requested.NR, data.frame(tmp2), by="NR.class")

allSmalls.requested.NR <- allSmalls.requested.NR[order(allSmalls.requested.NR$PWSID),]
#use inverse of response probabilitiy to generate non-response weight
allSmalls.requested.NR %<>% 
  mutate(wt.rr.final = 1/wt.rr) %>% 
  mutate(unwt.rr.final = 1/unwt.rr)
#display
allSmalls.requested.NR$wt.rr.final %>%
  unique %>%
  sort
```
Here we can see the range of weighted nonresponse weights for the non-imputed survey (without volunteers).We can see below that these weighted nonresponse weights are quite similiar to the unweighted nonreponse weights for the non-imputed survey.

```{r}
allSmalls.requested.NR$unwt.rr.final %>%
  unique %>%
  sort
```

These nonresponse weights are plotted relative to respective baseweights below.
```{r}
#plot versus base weights
ggplot(data = allSmalls.requested.NR, aes(x = base.weight, y = wt.rr.final, color = tag)) + geom_point() +
  labs(title = "Weighted Response Rates vs. Base Weights (non-imputed data)",
       xlab = "Base Weights",
       ylab = "Weighted Reponse Rates") +
  theme_minimal()
```

```{r}
# extract base weights and join to data
baseWeights <- allSmalls %>% 
  select(PWSID, base.weight, samples, N)

# compute NR adjustments based on classes formed by tree
# Unweighted response rate
unwt.rr <- by(as.numeric(imputedSmalls.requested.voluntary[, "response"]), t1$where, mean)

#join base weights to imputed
imputedSmalls.requested.voluntary <- left_join(imputedSmalls.requested.voluntary, baseWeights, by ="PWSID") 

# Weighted response rate
wt.rr <- by(data = data.frame(resp = as.numeric(imputedSmalls.requested.voluntary[, "response"]), wt =
                                imputedSmalls.requested.voluntary[,"base.weight"]),
            t1$where, function(x) {weighted.mean(x$resp, x$wt)} )
# merge NR class and response rates onto  file
imputedSmalls.requested.voluntary.NR <- cbind(imputedSmalls.requested.voluntary, NR.class=t1$where)
tmp1 <- cbind(NR.class=as.numeric(names(wt.rr)), unwt.rr, wt.rr)
imputedSmalls.requested.voluntary.NR <- merge(imputedSmalls.requested.voluntary.NR, data.frame(tmp1), by="NR.class")
imputedSmalls.requested.voluntary.NR <- imputedSmalls.requested.voluntary.NR[order(imputedSmalls.requested.voluntary.NR$PWSID),]
#use inverse of response probabilitiy to generate non-response weight
imputedSmalls.requested.voluntary.NR %<>% 
  mutate(wt.rr.final = 1/wt.rr) %>% 
  mutate(unwt.rr.final = 1/unwt.rr)

imputedSmalls.requested.voluntary.NR$wt.rr.final %>%
  unique %>%
  sort
```
The imputed data set contains slightly different weighted nonreponse weights.

```{r}
#plot versus base weights
ggplot(data = imputedSmalls.requested.voluntary.NR, aes(x = base.weight, y = wt.rr.final, color = tag)) + geom_point() +
  labs(title = "Weighted Response Rates vs. Base Weights (imputed data, with volunteers)",
       xlab = "Base Weights",
       ylab = "Weighted Reponse Rates") +
  theme_minimal()
```

If we had no information about population estimates, we would end the weighting procedure here. The ‘final weight’ would be the multiplication of both base scaled weight and the scaled non-response weight. Here we will call this new weights ‘final weights’ although we still have to perform adjustments to them and so will not really be ‘final’.

Before going to the next step we will include the computed non-response weights using adjustment classes to the main ‘data’ dataframe object. Then we will drop all non-respondents as we are not going to use them any more in the next steps of our analysis. After that, we will scale the non-response weights to the sample of respondents and multiply the design weights and the non-response weights.

```{r}
##### Join to non-imputed data #####
#join imputed smalls weights to full data frame
allSmalls %<>% 
  left_join(allSmalls.requested.NR %>% select(PWSID,  wt.rr, wt.rr.final, unwt.rr, unwt.rr.final),
            by = "PWSID")

#join non-response weights to just the sample frame
allSmalls.requested.responded %<>% 
  left_join(allSmalls.requested.NR %>% select(PWSID,  wt.rr, wt.rr.final, unwt.rr, unwt.rr.final),
            by = "PWSID")

#join the final weights to the respondent list
allSmalls.requested.responded %<>% 
  mutate(final.weight.nonresponse = wt.rr.final * base.weight) 

#view summary
allSmalls.requested.responded %>% 
  select(tag, PWSID, base.weight, final.weight.nonresponse, N, samples) %>% 
  group_by(tag) %>% 
  summarize(sum.final.weight = sum(final.weight.nonresponse), totalSamples = max(samples), population = max(N), percent.diff = 100*(1 - sum.final.weight/(population)))
```
Here we can see that the sum of the weights adjusted for non-response are slightly different from the population. This is fine as they are within 6%. The adjusted weights are plotted below:

```{r}
allSmalls.requested.responded %>% 
  ggplot(aes(x = base.weight, y =final.weight.nonresponse, color = Fee_Code, shape = tag)) +
  geom_point()
```
We can see clearly that the high non-response in the smaller bins has inflated the base weights. 

Repeat these steps for the imputed data.
```{r}
#join imputed smalls weights to imputed dataset
imputedSmalls %<>%
  left_join(imputedSmalls.requested.voluntary.NR %>% select(PWSID, wt.rr.final, unwt.rr, unwt.rr.final),
            by = "PWSID")

#join non-response weights to just the sample frame
imputedSmalls.requested.voluntary %<>% 
  left_join(imputedSmalls.requested.voluntary.NR %>% select(PWSID, wt.rr.final, unwt.rr, unwt.rr.final),
            by = "PWSID")

#join the final weights to the respondent list
imputedSmalls.requested.voluntary %<>% 
  mutate(final.weight.nonresponse = wt.rr.final * base.weight) %>% 
  mutate(final.weight.nonresponse.unwt = unwt.rr.final * base.weight)

#view summary
imputedSmalls.requested.voluntary %>% 
  select(tag, PWSID, base.weight, final.weight.nonresponse, final.weight.nonresponse.unwt, N, samples) %>% 
  group_by(tag) %>% 
  summarize(sum.final.weight = sum(final.weight.nonresponse), sum.final.weight.unwt.rr = sum(final.weight.nonresponse.unwt), totalSamples = max(samples), population = max(N), percent.diff = 100*(1 - sum.final.weight/(population)))

```
We can see that the dataset that included the volunteers dramatically inflates the nonresponse-adjusted weights. This is an unnaceptable difference.
```{r}
rm(imputedSmalls.requested.voluntary.NR, allSmalls.requested.NR, chk1, chk2, crf.prob2, crfsrvy.prob2, crf.srvy.2, crf.prob, crf.srvy, crfsrvy.prob, fitResponse, allSmalls.requested.response.model, allSmalls.simple, baseWeights, completenessSummary, log.reg.m, out, probSumm, t1, t2, tab, tmp1, tmp2, transposed, x.matrix, rpart.prob, predM, MissingMatrix, scaledSumm, smalls, srv.dsgn, rclus1)
```


## Weight Calibration

We've now generated *base weights* and *nonresponse adjustments* to those weights. The last step, which is extremely important in many surveys, is to use auxiliary data to correct coverage problems and to reduce standard errors.By auxiliary data, we mean information that is available for the entire frame or target population, either for each individual population unit or in aggregate form (i.e. househould income, service connections, population, fee code, regulating agency).Using auxiliary variable to adjust survey variables may reduce variances, but improve the representativeness of the survey.

Since our auxiliary variables are both quantitative(i.e. househould income, service connections, population) and qualitative (fee code, regulating agency), we will use a general regression estimator or GREG approach, which is a type of raking estimator. A GREG is approximately unbiased in repeated sampling if the frame provides full coverage of the target population (in this case we have census data for these auxiliary variables) ( [Vallian et al 2018](http://link.springer.com/10.1007/978-3-319-93632-1)).

### Evaluation of Correlative factors

The first step in selecting appropriate auxiliary variables to use for raking is to deterimine correlations with response variables of interest. This can be achieved using a scatterplot matrix of the quantitative variables in the dataset.
```{r}
require(psych)
#trim data
aux.response <- allSmalls.requested.responded %>% select(Service_Connections,Median_12month_HH_income, delinquent_num_acc, months_before_assist_num, cash_reserve_total)
# plot matrix
pairs.panels(aux.response,
             method = "pearson",
             hist.col = "#00AFBB", # color
             density = TRUE, #show histograms
             ellipses = TRUE,#annotate correlation elliopses
             stars = TRUE) #show significance)
```

Here we can see that all of the quantitative auxiliary variables (Service_Connections, Median_12month_HH_income) are correlated with the selected response variables (delinquent number of accounts, months before assistance needed, cash reserve total). These are appropriate variables to use for raking. Let's look a little closer at the relationships between service connections and delinquent number of accounts and total cash reserve.
```{r}
require(psych)
#trim data
aux.response <- allSmalls.requested.responded %>% 
  select(Service_Connections, delinquent_num_acc, cash_reserve_total)
# plot matrix
pairs.panels(aux.response,
             method = "pearson",
             hist.col = "#00AFBB", # color
             density = TRUE, #show histograms
             ellipses = TRUE,#annotate correlation elliopses
             smoother = TRUE,
             stars = TRUE) #show significance)
```
We can see that service connections is a useful linear predictor for delinquent number of accounts (r = 0.52) and total cash reserve (r = 0.47). Interestingly, the delinquent number of accoutns and total cash reserves has a very non-linear relationship, resembling a volcano plot.

It may be better to visualize  the number of delinquent accounts and cash totals on a log10 scale.
```{r}
#trim data
aux.response_log <- allSmalls.requested.responded %>% 
  select(Service_Connections, delinquent_num_acc, cash_reserve_total) %>% 
   mutate(log.d.n.acc = log10(delinquent_num_acc)) %>% 
  mutate(log.cash.total = log10(cash_reserve_total)) %>% 
  drop_na() %>% 
  filter(log.d.n.acc > 0) %>% 
  filter(log.cash.total > 0) %>% 
  select(Service_Connections, log.d.n.acc, log.cash.total)
# plot matrix
pairs.panels(aux.response_log,
             method = "pearson",
             hist.col = "#00AFBB", # color
             density = TRUE, #show histograms
             ellipses = TRUE,#annotate correlation elliopses
             smoother = TRUE,
             stars = TRUE) #show significance)
 
```
After log transformation we can see more linear relationships between the predictor (service connections) and response variables. Delinquent number of accounts normalized to service connections is a critical response variable that may be predicted from auxiliary variables. 

```{r}
#trim data
aux.response <- allSmalls.requested.responded %>% 
  mutate(logServiceConnections = log10(Service_Connections)) %>% 
  select(Service_Connections, delinquent_num_acc_normalized, Median_12month_HH_income, months_before_assist_num)
# plot matrix
pairs.panels(aux.response,
             method = "pearson",
             hist.col = "#00AFBB", # color
             density = TRUE, #show histograms
             ellipses = TRUE,#annotate correlation elliopses
             smoother = TRUE,
             stars = TRUE) #show significance)
 
```

Here we can see that service connections and median 12 month household income are not good predictors of the service-connection *normalized* number of delinquent accounts. However, a strong relationship exists between the *normalized* number of delinquent accounts and the qualitative variable for months before assistance needed. This is not useful for raking, as neither variables are auxiliary, however this is an interesting (and expected) correlation. Again, median 12 month household income and months before assistance needed are significantly correlated. 

To continue raking, we will now perform more formal analyses. Using all reliable, complete auxiliary data present, let's see the linear relationships to the continuous variable months before assistance needed (from factor)
```{r}
m2 <- glm(months_before_assist_num ~ Service_Connections + Median_12month_HH_income + Median_rent_pct_income + Fee_Code, na.action = na.exclude, data = allSmalls.requested.responded)
summary(m2)
```
Again we can see that median 12 month household income is a strong linear predictor for months before assistance needed. A non-significant trend exists between disadvantaged smalls and months before assistance (as would be expected). 

The relationship between fee code and months before assistance needed can be visualized as a boxplot. 

```{r}
p1 <- allSmalls.requested.responded %>% 
  ggplot(aes(x = Fee_Code, y = months_before_assist_num, fill = Fee_Code)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = "none")
p2 <- allSmalls.requested.responded %>% 
  ggplot(aes(x = Median_12month_HH_income, y = months_before_assist_num, color = Fee_Code)) +
  geom_point(position = "jitter", alpha = 0.6) +
  theme_minimal()

grid.arrange(p1, p2,ncol = 2,
             top = textGrob("Months Before Assistance Needed by Significant Predictive Factors",
                            gp=gpar(fontsize = 22,font=6)))
```
We can see quite clearly the relationship between fee code and months before assistance needed, with the disadvantaged community larges representing the most at-risk overall, and the community smalls representing lowest risk. Again remember that these values are unweighted and are only used for selecting auxiliary variables for raking.Nonetheless, the relationship between median 12 month household income and months before assistance needed seems to be quite robust. There are, however, some outliers that may negatively affect raking. We may choose to remove these outliers for raking purposes.

```{r}
require(car)
#glm for only predictive variables
m3 <- glm(months_before_assist_num ~ Median_12month_HH_income + Fee_Code,na.action = na.exclude,  data = allSmalls.requested.responded)
#  Bonferonni p-value for most extreme obs
outlierTest(m3)
```
It seems that three values are extreme outliers.
```{r, results='hide'}
#code row number
allSmalls.requested.responded %<>% 
  mutate(id = row_number())
#examine cases
allSmalls.requested.responded %>%
  filter(id == 305 | id == 341 | id == 259)
```
Here we can see these are quite small systems. The Fisherman's retreat in Riverside, Belmont manor in Fresno, and Wayside motel apparments in San Joaquin county. The notes indicate that the last one does not have financial information for the water system and the first is just for seven months of expenses. The first and third report no cash reserves. The third reports that many people are not paying rent.All are in relatively moderate income areas (78-86 k/year median). These are important data points that should be considered later, but may innapropriately skew the raking process.

Let's look at residuals for predictive variables.
```{r}
summary(m3)
```
```{r}
residualPlots(m3, main="QQ Plot") #qq plot for studentized resid
```

Since there are considerable deviant residuals for this model, let's try a more simple model with an intuitive and likely simpler dependent variable (total cash reserves).
```{r}
#create simple dataset for modeling that drops missing values and provides log transform 
simpleModel <- allSmalls.requested.responded %>% 
  select(PWSID, base.weight, final.weight.nonresponse, Service_Connections, delinquent_num_acc, cash_reserve_total, Fee_Code, Median_12month_HH_income) %>% 
   mutate(log.d.n.acc = log10(delinquent_num_acc)) %>% 
  mutate(log.cash.total = log10(cash_reserve_total)) %>% 
  drop_na() %>% 
  filter(log.d.n.acc > 0) %>% 
  filter(log.cash.total > 0) %>% 
  select(Service_Connections, log.d.n.acc, log.cash.total, PWSID, base.weight, final.weight.nonresponse, Service_Connections, delinquent_num_acc, cash_reserve_total, Fee_Code, Median_12month_HH_income) %>% 
  droplevels()
#build glm
m5 <- glm(log.cash.total ~ Service_Connections + Median_12month_HH_income  + Fee_Code, na.action = na.exclude, data = simpleModel)
#report
summary(m5)
```
Here we can see a much simpler model with a lower AIC (380.65) compared to the models that tried to predict months before assistance needed (AIC = 1079.7). We will use this simple relationship to calibrate the data. Let's see this highly correlated relationship in graphical form.

```{r}
p1 <- simpleModel %>% 
  ggplot(aes(x = log10(Service_Connections), y = log.cash.total)) +
  geom_point() +
  geom_smooth()+
  theme_minimal() 

p2 <- simpleModel %>% 
  ggplot(aes(x = log10(Service_Connections), y = log.cash.total, color = Fee_Code)) +
  geom_point() +
  theme_minimal() 
grid.arrange(p1, p2)
```
Here we can see the strong linear relationship between cash total and service connections, which will form the basis for building a model to rake the data.

### Calibration

#### Small Systems
The code below uses the *sampling* package to select a sample with probability proportional to the median 12 month household income . The method of selection is to randomize the order of the population and then select a systematic sample (see Hartley and Rao 1962).
```{r}
#get population totals for items of interest
N.PS <- xtabs(~Fee_Code, data = allSmalls)
```


```{r}
require(survey)
#build survey design with no weights
srv.dsgn.unweighted <- svydesign(ids = ~0,
                                 strata = ~tag,
                                 data = allSmalls.requested.responded,
                                 fpc = ~fpc,
                                 weights = ~NULL)
#build survey design with non-adjusted  weights
srv.dsgn.unadjusted <- svydesign(ids = ~ 0, # no clusters
                      strata = ~tag,
                      data = allSmalls.requested.responded,
                      fpc = ~fpc,
                      weights = ~base.weight)
#build survey design with adjusted non-response weights
srv.dsgn <- svydesign(ids = ~ 0, # no clusters
                      strata = ~tag,
                      data = allSmalls.requested.responded,
                      fpc = ~fpc,
                      weights = ~final.weight.nonresponse)
#PostStratify by fee code
ps.dsgn <- postStratify(design = srv.dsgn,
                        strata = ~Fee_Code,
                        partial = TRUE,
                        population = N.PS)

#IMPUTED WITH VOLUNTEERS
#filter non-responses
imputedSmalls.survey <- imputedSmalls.requested.voluntary %>% filter(responded == "y")

fpcID <- allSmalls %>% select(PWSID, fpc)
imputedSmalls.survey <- left_join(imputedSmalls.survey, fpcID)
#Build survey design with volunteer data and imputed values
srv.dsgn.imputed.volunteers <- svydesign(ids = ~ 0, # no clusters
                                         strata = ~tag,
                                         data = imputedSmalls.survey,
                                         fpc = ~fpc,
                                         weights = ~base.weight) #notice that base weights are used isntead of nonresponse adjusted as they dramatically overestimate

#postStratify with volunteer data and imputed values
ps.dsgn.imputed.volunteers <- postStratify(design = srv.dsgn.imputed.volunteers,
                                           strata = ~Fee_Code,
                                           partial = TRUE,
                                           population = N.PS)
```

```{r}
#if multiple post-strata are used, check that weights are calibrated
svytotal(~interaction(Fee_Code, tag), ps.dsgn)
```
Let's compare the weights calculated from post-stratification with the non-reponse-adjusted weights and the volunteer imputed survey weights.

```{r}
#examine new weights
rbind(summary(weights(srv.dsgn)), summary(weights(ps.dsgn)), summary(weights(ps.dsgn.imputed.volunteers)))
```
We can see that post-stratification by fee code alone did not alter the weights significantly. This is not surprising due to the original survey design. However, significantly smaller weights are avilable for the post-stratified imputed survey set with volunteers (line 3).

The estimated proportion of fee codes and their their SEs are reported below for the post-stratified.
```{r}
svytotal(~Fee_Code, ps.dsgn, na.rm = TRUE)
```

Coefficients of variation produced by the post-stratification are provided below.
```{r}
cv(svytotal(~Fee_Code, ps.dsgn, na.rm = TRUE))
```
If we compare these values to those in the original survey design (without post-stratification), we can see the differences in totals and standard errors.
```{r}
svytotal(~Fee_Code, srv.dsgn, na.rm = TRUE)
```
```{r}
cv(svytotal(~Fee_Code, srv.dsgn, na.rm = TRUE))
```

Since we also post-stratified the imputed data, we can view the total estimaes as well. 
```{r}
svytotal(~Fee_Code, ps.dsgn.imputed.volunteers, na.rm = TRUE)
```

<!-- Another method is available, termed 'raking.' -->
<!-- ```{r} -->
<!-- #use raking method -->
<!-- srv.dsgn.rake <- rake(design = srv.dsgn, -->
<!--                       sample.margins = list(~Fee_Code), -->
<!--                       population.margins = list(fee.dist)) -->
<!-- ``` -->



Let's see how the change in weights changes means for response variables.
```{r}
#calcualte unweighted means
srv.mean.unweighted <- svymean(~cash_reserve_total + cash_reserve_restricted + cash_reserve_unrestricted +
                         months_before_assist_num + delinquent_num_acc + 
                           delinquent_amount_dollars, 
                         srv.dsgn.unweighted, na.rm = TRUE)
srv.mean.unweighted <- data.frame(srv.mean.unweighted)
#reassign names
srv.mean.unweighted <- setNames(cbind(rownames(srv.mean.unweighted), srv.mean.unweighted, row.names = NULL),
         c("variable", "srv.Mean.unweighted", "srv.SE.unweighted")) 

#calcualte un-adjusted weights means
srv.mean.unadjusted <- svymean(~cash_reserve_total + cash_reserve_restricted + cash_reserve_unrestricted +
                         months_before_assist_num + delinquent_num_acc + 
                           delinquent_amount_dollars, 
                         srv.dsgn.unadjusted, na.rm = TRUE)
srv.mean.unadjusted <- data.frame(srv.mean.unadjusted)
#reassign names
srv.mean.unadjusted <- setNames(cbind(rownames(srv.mean.unadjusted), srv.mean.unadjusted, row.names = NULL),
         c("variable", "srv.Mean.unadjusted", "srv.SE.unadjusted")) 

#calcualte survey means
srv.mean <- svymean(~cash_reserve_total + cash_reserve_restricted + cash_reserve_unrestricted +
                         months_before_assist_num + delinquent_num_acc + 
                           delinquent_amount_dollars, 
                         srv.dsgn, na.rm = TRUE)
srv.mean <- data.frame(srv.mean)
#reassign names
srv.mean <- setNames(cbind(rownames(srv.mean), srv.mean, row.names = NULL),
         c("variable", "srv.Mean", "srv.SE")) 
#calcualte post-stratification means
post.strat.mean  <-  svymean(~cash_reserve_total + cash_reserve_restricted +
                               cash_reserve_unrestricted +
                         months_before_assist_num + delinquent_num_acc + 
                           delinquent_amount_dollars,
                         ps.dsgn, na.rm = TRUE)

post.strat.mean <- data.frame(post.strat.mean)
post.strat.mean <- setNames(cbind(rownames(post.strat.mean), post.strat.mean, row.names = NULL),
         c("variable", "post.strat.Mean", "post.strate.SE"))

#calcualte post-stratified means for IMPUTED data
srv.mean.imputed.volunteers <- svymean(~cash_reserve_total + cash_reserve_restricted + cash_reserve_unrestricted
                                       +
                         months_before_assist_num + delinquent_num_acc + 
                           delinquent_amount_dollars, 
                         srv.dsgn.imputed.volunteers, na.rm = TRUE)
srv.mean.imputed.volunteers <- data.frame(srv.mean.imputed.volunteers)
#reassign names
srv.mean.imputed.volunteers <- setNames(cbind(rownames(srv.mean.imputed.volunteers), srv.mean.imputed.volunteers, row.names = NULL),
         c("variable", "srv.mean.imputed.volunteers", "srv.SE.imputed.volunteers")) 


#join tables
left_join(left_join(left_join(left_join(srv.mean,post.strat.mean, by = "variable"), srv.mean.unadjusted),srv.mean.unweighted), srv.mean.imputed.volunteers)
```
We can see that adjusted for non-response and post-stratifying by fee code has increased the standard error and the mean relative to no adjustment for most variables. 

```{r}
# construct and display a frequency table STRATIFIED
tab_fee_need_cond <- svytable(~Fee_Code + months_before_assist,
                         design = ps.dsgn) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
# repeate for unweighted
tab_fee_need_cond_unwt <- svytable(~Fee_Code + months_before_assist,
                         design = srv.dsgn.unweighted) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
#repeat for volunteers and imputed data
tab_fee_need_cond_imputed <- svytable(~Fee_Code + months_before_assist,
                         design = srv.dsgn.imputed.volunteers) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
```

```{r}
# Create a segmented bar graph of the conditional proportions in table
p1 <- ggplot(data = tab_fee_need_cond,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Post-Stratified Months Before Assistance by Fee Code",
       xlab = "Proportion")
#before strat
p2 <- ggplot(data = tab_fee_need_cond_unwt,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Months Before Assistance by Fee Code (Pre-Stratification)",
       xlab = "Proportion") +
  theme(legend.position = "none")
# Imputed with volunteers
p3 <- ggplot(data = tab_fee_need_cond_imputed,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Months Before Assistance by Fee Code (Imputed, w/Volunteers)",
       xlab = "Proportion") +
  theme(legend.position = "none")

grid.arrange(p1, p2, p3)
```
We can see that the weighing does not apprecitably change the results. However there does seem to be a slight change in the number of disadvantaged community larges with immediate need in the volunteer group with imputation.

```{r}
# Create a segmented bar graph of the conditional proportions in table
p1 <- tab_fee_need_cond %>% 
  filter(months_before_assist == "A") %>% 
  ggplot(mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() +

  labs(title = "Post-Stratified Months Before Assistance by Fee Code",
       xlab = "Proportion")+
  theme(legend.position = "none")
#before strat
p2 <- tab_fee_need_cond_unwt %>% 
  filter(months_before_assist == "A") %>% 
  ggplot(
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() + 
  labs(title = "Months Before Assistance by Fee Code (Pre-Stratification)",
       xlab = "Proportion") +
  theme(legend.position = "none")
# Imputed with volunteers
p3 <- tab_fee_need_cond_imputed %>% 
   filter(months_before_assist == "A") %>% 
  ggplot(
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = months_before_assist)) + 
  geom_col() + 
  labs(title = "Months Before Assistance by Fee Code (Imputed, w/Volunteers)",
       xlab = "Proportion") +
  theme(legend.position = "none")

grid.arrange(p1, p2, p3)
```
Our survey has now been weighted to adequately represent fee code distributions.

Now that we have our adjusted weights, let's extract them and bind to the dataframes.
```{r}
#bind post-stratified weights to all smalls (no volunteers)
allSmalls.requested.responded <- cbind(allSmalls.requested.responded, weights(ps.dsgn))
#bind post-stratified weights to imputed survey with volunteers
imputedSmalls.survey <- cbind(imputedSmalls.survey, weights(ps.dsgn.imputed.volunteers))

#Save to csv
# write.csv(x = allSmalls.requested.responded %>% 
#              select(PWSID,`weights(ps.dsgn)`),
#            file = "Datasets/noVolunteerSurveyWEIGHTS.csv"
#            )

#Save to csv
# write.csv(x = imputedSmalls.survey %>% 
#             select(PWSID,`weights(ps.dsgn.imputed.volunteers)`),
#           file = "Datasets/fullSurveyWEIGHTS.csv"
#           )
#Save to csv
write.csv(x = imputedSmalls.survey,
           file = "Datasets/fullSurveyIMPUTED.csv")

```

#### Large systems
The above steps are repeated for large systems, using service connections asa proxy for representativeness. Jenk's Natural Breaks are used to bin systems naturally by service connections for calibration.

```{r}
#jenk's breaks for larges
#determine mean and stdev
allLarges %>% 
  summarize(mean = mean(Service_Connections),
            var = var(Service_Connections))
```
Above we can see the mean and variance of service ceonntions for large systems. This is visualized as histrograms.
```{r}
allLarges %>% 
  ggplot(aes(x = log10(Service_Connections), fill = responded)) +
  geom_histogram()
```
Natural breaks are determined using the Jenk's method.
```{r}
#### Natural Breaks ####
#Determine natural breaks and assign
breaks <- getJenksBreaks(allLarges$Service_Connections, 4)

#specify bin labels
postStrata <- c("A", "B", "C")#, "D")#, "E", "F")#, "G", "H", "I")

#bucket values into bins
bins <- cut(allLarges$Service_Connections,
            breaks = breaks,
            include.lowest = TRUE,
            right = FALSE,
            labels = postStrata)

#inspect bins
summary(bins)
```
A summary of the number of systems in each bin is shown above,.
```{r}
breaks
```
The discrete breaks for service connections is above. The next chunk joins these breaks to the dataset.
```{r}
#Store group as new column
allLarges <-as_tibble(allLarges) %>% 
  mutate(postStrata = case_when(
    Service_Connections >= breaks[1] & Service_Connections < breaks[2] ~postStrata[1],
    Service_Connections >= breaks[2] & Service_Connections < breaks[3] ~postStrata[2],
    Service_Connections >= breaks[3] & Service_Connections <= breaks[4] ~postStrata[3],
  ))

#tag is character vector, so convert to factor
allLarges$postStrata <- factor(allLarges$postStrata,
                    ordered = FALSE)
```

Now that we have defined strata to adjust the data with, calibration can be carried out. There are two fee codes for larges (C1 and DAVCL). These will also be used to calibrate the surveys.
```{r}
#get population totals for items of interest
N.PS <- xtabs(~Fee_Code, data = allLarges)
N.SC <- xtabs(~postStrata, data = allLarges)
#combined
N.PS.SC <- xtabs(~Fee_Code + postStrata, data = allLarges)
```


```{r}
require(survey)
#separate samples from population
larges <- allLarges %>% 
  filter(responded == "y")
#build survey design with no weights
srv.dsgn.unweighted <- svydesign(ids = ~0,
                                 strata = ~tag,
                                 data = larges,
                                 fpc = ~fpc,
                                 weights = ~NULL)
#build survey design with non-adjusted  weights
srv.dsgn.unadjusted <- svydesign(ids = ~ 0, # no clusters
                      strata = ~tag,
                      data = larges,
                      fpc = ~fpc,
                      weights = ~base.weight)
# #build survey design with adjusted non-response weights
# srv.dsgn <- svydesign(ids = ~ 0, # no clusters
#                       strata = ~tag,
#                       data = allSmalls.requested.responded,
#                       fpc = ~fpc,
#                       weights = ~final.weight.nonresponse)
#PostStratify by fee code
ps.dsgn.fee.code <- postStratify(design = srv.dsgn.unadjusted,
                        strata = ~Fee_Code,
                        partial = TRUE,
                        population = N.PS)
#Post stratify further by service connections
ps.dsgn.fee.code.SC <- postStratify(design = ps.dsgn.fee.code,
                                    strata = ~postStrata,
                                    partial = TRUE,
                                    population = N.SC)
#do both together
ps.dsgn.larges <- postStratify(design = srv.dsgn.unadjusted,
                                    strata = ~Fee_Code + postStrata,
                                    partial = TRUE,
                                    population = N.PS.SC)

```

Weights have now been adjusted for fee codes and service connections. The interaction of these weights are seen below.

```{r}
#if multiple post-strata are used, check that weights are calibrated
svytotal(~interaction(Fee_Code, postStrata), ps.dsgn.larges)
```

Let's compare the weights calculated from post-stratification with the non-reponse-adjusted weights and the volunteer imputed survey weights.

```{r}
#examine new weights
rbind(summary(weights(srv.dsgn.unadjusted)),summary(weights(ps.dsgn.fee.code)), summary(weights(ps.dsgn.fee.code.SC)), summary(weights(ps.dsgn.larges)))
```
We can see that post-stratification by fee code alone (second row) did not alter the weights significantly.Further post-stratification by service connection natural breaks increased the range of weights. The final row is the completed design.

The estimated proportion of fee codes and their their SEs are reported below for the post-stratified.
```{r}
svytotal(~Fee_Code, ps.dsgn.larges, na.rm = TRUE)
```

Coefficients of variation produced by the post-stratification are provided below.
```{r}
cv(svytotal(~Fee_Code, ps.dsgn.larges, na.rm = TRUE))
```
If we compare these values to those in the original survey design (without post-stratification), we can see the differences in totals and standard errors.
```{r}
svytotal(~Fee_Code, srv.dsgn.unadjusted, na.rm = TRUE)
```
A marked decrease in variance is noted with post-stratificaiton.
```{r}
cv(svytotal(~Fee_Code, srv.dsgn.unadjusted, na.rm = TRUE))
```


```{r}
# construct and display a frequency table STRATIFIED
tab_fee_need_cond <- svytable(~Fee_Code + late_fees_YN,
                         design = ps.dsgn.larges) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
# repeate for unadjusted weights
tab_fee_need_cond_unwt <- svytable(~Fee_Code + late_fees_YN,
                         design = srv.dsgn.unadjusted) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
```

```{r}
# Create a segmented bar graph of the conditional proportions in table
p1 <- ggplot(data = tab_fee_need_cond,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = late_fees_YN)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Late Fees (Post-Stratifed)",
       xlab = "Proportion")
#before strat
p2 <- ggplot(data = tab_fee_need_cond_unwt,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = late_fees_YN)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Late Fees (unadjusted weights)",
       xlab = "Proportion")

grid.arrange(p1, p2)
```

Now to ensure adjustedweights sum up to the entire population from which each population was drawn.
```{r}
#bind post-stratified weights to larges
larges <- cbind(larges, weights(ps.dsgn.larges))
  
larges %<>% rename(c("final.weight" = `weights(ps.dsgn.larges)`))
```

Now that we have our adjusted weights, let's extract them and bind to the dataframes.
```{r}
#Save to csv
write.csv(x = larges,
           file = "Datasets/fullSurveyLarges.csv")
```


<!-- #### Raking  -->
<!-- NOTE THAT THIS SECTION IS NOT COMPLETED -->
<!-- The distribution for median household income are sampled so that it is properly represented. Next, the *survey* package is used to create a design object, rakedf.dsgn, that is then used in the calibrate function to compute GREG weights. -->

<!--  ```{r} -->
<!--  rakedf.dsgn <- svydesign(ids = ~0, # no clusters -->
<!--                         strata = ~tag, # original strata -->
<!--                         data = allSmalls.requested.responded, -->
<!--                         weights = ~base.weight) #original weights -->
<!--  # Compute pop totals of auxiliaries -->
<!--  # Note these are the original not the recoded x’s -->
<!--  #number of service connections -->
<!--  # x.service_connections <- by(allSmalls$Service_Connections, allSmalls$Fee_Code, sum) -->
<!--  x.service_connections <- sum(allSmalls$Service_Connections) -->

<!--  # population -->
<!--  #x.pop <- sum(allSmalls$Population) -->
<!--  x.Median_12month_HH_income <- sum(allSmallsdf$Median_12month_HH_income) -->
<!--  N <- nrow(allSmalls) -->

<!--  pop.tots <- c('(Intercept)' = N, -->
<!--                 #Population = x.pop, -->
<!--                 Service_Connections = x.service_connections, -->
<!--                Median_12month_HH_income = x.Median_12month_HH_income) -->
<!--  #manually rename combos -->
<!--  # names(pop.tots) <- c("(Intercept)", -->
<!--  #                      #"Population", -->
<!--  #                      "Service_Connections:Fee_CodeC1", -->
<!--  #                      "Service_Connections:Fee_CodeDAVCL", "Service_Connections:Fee_CodeDAVCS", -->
<!--  #                      "Service_Connections:Fee_CodeSC", "Median_12month_HH_income" ) -->

<!--  sam.lin <- calibrate(design = rakedf.dsgn, -->
<!--                       formula = ~Median_12month_HH_income + -->
<!--                         #Population + -->
<!--                         Service_Connections, -->
<!--                       population = pop.tots, -->
<!--                       #lower and upper bounds for weights. upper based on max base weight -->
<!--                       maxit = 500, -->
<!--                       bounds = c(-Inf, Inf), -->
<!--                       epsilon = 1e-6, -->
<!--                       calfun="linear") -->
<!--  ``` -->
<!--  The parameter setting calfun=c("linear") results in GREG weights being computed. As in oststratification and raking, we can check whether the calibration constraints were satisfied: -->
<!--  ```{r} -->
<!--  svyby(~Service_Connections, by=~Fee_Code, design=sam.lin, FUN=svytotal) -->
<!--  ``` -->

<!--  ```{r} -->
<!--  svytotal(~Median_12month_HH_income, sam.lin) -->
<!--  ``` -->

<!--  ```{r} -->
<!--  svytotal(~Population, sam.lin) -->
<!--  ``` -->
<!--  Since the SEs are essentially or exactly 0, a set of satisfactory weights has been obtained. xamining summary statistics for the weights is wise. -->

<!--  ```{r} -->
<!-- summary(weights(rakedf.dsgn)) -->
<!--  ``` -->

<!--  ```{r} -->
<!--  summary(weights(sam.lin)) -->
<!--  ``` -->




<!-- ## Calibration Weighting -->
<!-- Since population totals for a set of auxiliary variables (e.g. service connections, regulating agency, fee code, median income) are known, calibration weighting is useful for deriving weights that can be used to calculate generalizable estimators from non-representative data ( [Deville, J.-C. and C.-E. S¨arndal 1992](https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475217). As such, it has a lengthy history as a tool for non-response adjustments (e.g. [Kott, 2006](https://www.rti.org/publication/using-calibration-weighting-adjust-nonresponse-and-coverage-errors)) and has a similar utility for blending probability and convenience samples. Further, it should give results that are similar to those found using propensity score weighting (albeit through different computational procedures). -->

<!-- Calibration weighting (commonly referred to as generalized raking) calculates the set of calibrated -->
<!-- weightswhich minimize the distance between the initial and calibrated weights ( [Robbins et al 2019](https://arxiv.org/pdf/1908.04217.pdf)). Since non-response is a factor for this survey, propensity score-based weights can and should be used as initial values within calculation of calibration weights. The somewhat novel technique known as **simultaneous calibration** generates weights that jointly blend the probability and non-probability (i.e. volunteer) samples to produce a blended sample that is representative of the population.  -->

<!-- ```{r} -->
<!-- require(NonProbEst) -->
<!-- #make sub-samples with selected values -->
<!-- allSmalls.volunteers.noNA <- allSmalls.volunteers %>%  -->
<!--   select(months_before_assist, Service_Connections, Fee_Code, Median_12month_HH_income, Median_rent_pct_income) %>%   -->
<!--   drop_na() -->

<!-- allSmalls.requested.noNA <- allSmalls %>%  -->
<!--   filter(requested == "y") %>%  -->
<!--   select(months_before_assist, Service_Connections, Fee_Code, Median_12month_HH_income, Median_rent_pct_income) %>%  drop_na() -->


<!-- ``` -->




# Summary Statistics
Now that we have our adjusted weights and a complete (imputed) dataset, it's time to compute summary statistics (totals, means, variances, ratios, and quantiles).Estimates for statewide number of delinquent accounts for smalls and larges are calcualted below.  

```{r}
#smalls
svyby(~delinquent_amount_dollars, 
         by = ~Fee_Code, 
         design = ps.dsgn,
      FUN = svytotal,
         na.rm = TRUE)
```
Above is for smalls. Below is for larges for total dollars of delinquent accouts (including other expenses such as power).
```{r}
#larges
svyby(~dollars_del_acc_TotR, 
         by = ~Fee_Code, 
         design = ps.dsgn.larges,
      FUN = svytotal,
         na.rm = TRUE)
```
Below is for larges for total dollars of delinquent accouts (drinking water alone).
```{r}
#larges
svyby(~dollars_del_dw_TotR, 
         by = ~Fee_Code, 
         design = ps.dsgn.larges,
      FUN = svytotal,
         na.rm = TRUE)
```

Now simply totals without binning.
```{r}
#smalls
svytotal(~delinquent_amount_dollars, 
         design = ps.dsgn,
         na.rm = TRUE)
```
Above is totals for smalls. 57,595,916 +- 2,470,236.

Below is for larges (all expenses)
```{r}
#smalls
svytotal(~dollars_del_acc_TotR, 
         design = ps.dsgn.larges,
         na.rm = TRUE)
```
And below for just drinking water expenses.

```{r}
#smalls
svytotal(~dollars_del_dw_TotR, 
         design = ps.dsgn.larges,
         na.rm = TRUE)
```

All of these data can be viewed as a table. 
```{r}
# construct and display a frequency table STRATIFIED
tab_debt_cond_lrg <- svytable(~Fee_Code + dollars_del_acc_TotR,
                         design = ps.dsgn.larges) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
# repeate for unadjusted weights
tab_debt_cond__lrg_unwt <- svytable(~Fee_Code + dollars_del_acc_TotR,
                         design = srv.dsgn.unadjusted) %>%
  # Add conditional proportions to table
    as.data.frame() %>%
    group_by(Fee_Code) %>%
    mutate(n_Fee_Code = sum(Freq), Prop_Need = Freq/n_Fee_Code) %>%
    ungroup()
```

```{r}
# Create a segmented bar graph of the conditional proportions in table
p1 <- ggplot(data = tab_fee_need_cond,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = late_fees_YN)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Late Fees (Post-Stratifed)",
       xlab = "Proportion")
#before strat
p2 <- ggplot(data = tab_fee_need_cond_unwt,
       mapping = aes(x = Fee_Code, y = Prop_Need, fill = late_fees_YN)) + 
  geom_col() + 
  coord_flip() +
  labs(title = "Late Fees (unadjusted weights)",
       xlab = "Proportion")

grid.arrange(p1, p2)
```

Let's ask what the mean cash reserve total is for each of the strata. The mean for each bin is interpreted as the proportion for each category.


```{r summary statistics}
# svymean(~cash_reserve_total + tag, dclus1)
```




 7. What is the total amount of household water debt accumulated since the beginning of the COVID-19 emergency? [MODERATE, REQUIRES WEIGHTING, COMBINING WITH ALL SYSTEMS]